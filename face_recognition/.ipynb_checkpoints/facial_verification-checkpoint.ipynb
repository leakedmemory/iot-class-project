{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c6c5d3-f1ec-4d7a-b5bb-b497ed1ed165",
   "metadata": {},
   "source": [
    "## Setup Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "146c52c0-3807-4bb3-8d4f-10256f620dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 20:36:13.255269: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 20:36:17.628414: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-07 20:36:17.628488: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-07 20:36:17.628495: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-07 20:36:20.894824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 20:36:20.937741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 20:36:20.937967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "POS_PATH = os.path.join(\"data\", \"positive\")\n",
    "NEG_PATH = os.path.join(\"data\", \"negative\")\n",
    "ANC_PATH = os.path.join(\"data\", \"anchor\")\n",
    "\n",
    "os.makedirs(POS_PATH, exist_ok=True)\n",
    "os.makedirs(NEG_PATH, exist_ok=True)\n",
    "os.makedirs(ANC_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b870aa-992d-47d7-a967-2a9a58e35f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Siamese L1 Distance\n",
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "    # Similarity calculation\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)\n",
    "\n",
    "\n",
    "custom_objects = {\"L1Dist\": L1Dist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb751e28-b3ab-4e9c-a417-adb738de6c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 20:36:21.718785: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 20:36:21.719588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 20:36:21.719953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 20:36:21.720160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 20:36:22.233248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 20:36:22.233443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 20:36:22.233591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 20:36:22.233715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2564 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"siamesemodel.h5\"\n",
    "siamese_model = None\n",
    "if os.path.isfile(MODEL_PATH):\n",
    "    siamese_model = tf.keras.models.load_model(\n",
    "        MODEL_PATH,\n",
    "        custom_objects={\"L1Dist\": L1Dist}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29414b4b-8229-4000-9c32-4090c5c6fa19",
   "metadata": {},
   "source": [
    "## Collect Positives and Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eb168d7-bffa-4e6f-86dc-966d6a2fbec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping downloading, extracting and moving the dataset\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "if len(os.listdir(NEG_PATH)) == 0:\n",
    "    ds_compressed_path = \"lfw.tgz\"\n",
    "    ds_directory_path = \"lfw\"\n",
    "\n",
    "    # download and extract Labelled Faces in the Wild dataset\n",
    "    urllib.request.urlretrieve(\"http://vis-www.cs.umass.edu/lfw/lfw.tgz\", ds_compressed_path)\n",
    "\n",
    "    with tarfile.open(ds_compressed_path) as file:\n",
    "        file.extractall()\n",
    "\n",
    "    # move dataset images to the `NEG_PATH` directory\n",
    "    for directory in os.listdir(ds_directory_path):\n",
    "        for file in os.listdir(os.path.join(ds_directory_path, directory)):\n",
    "            ex_path = os.path.join(ds_directory_path, directory, file)\n",
    "            new_path = os.path.join(NEG_PATH, file)\n",
    "            os.replace(ex_path, new_path)\n",
    "\n",
    "    # remove dataset compressed file and directory\n",
    "    shutil.rmtree(ds_directory_path)\n",
    "    os.remove(ds_compressed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa20335-fb97-44cd-a4cb-20f2b4c65252",
   "metadata": {},
   "source": [
    "### Collect Positive and Anchor classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a210ba3b-e4eb-4ef4-baac-6ebf1ed73a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping positive and anchor collection section\n"
     ]
    }
   ],
   "source": [
    "%%script echo \"Skipping positive and anchor collection section\" --no-raise-error\n",
    "# Comment the line above to execute the cell\n",
    "\n",
    "import cv2\n",
    "import uuid\n",
    "\n",
    "cap = cv2.VideoCapture(2)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Cut frame to 250x250 px\n",
    "    frame = frame[150:150+250, 200:200+250, :]\n",
    "    \n",
    "    # Collect anchors\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"a\"):\n",
    "        image_name = os.path.join(ANC_PATH, f\"{uuid.uuid1()}.jpg\")\n",
    "        cv2.imwrite(image_name, frame)\n",
    "    \n",
    "    # Collect positives\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"p\"):\n",
    "        image_name = os.path.join(POS_PATH, f\"{uuid.uuid1()}.jpg\")\n",
    "        cv2.imwrite(image_name, frame)\n",
    "    \n",
    "    # Show taken image on screen\n",
    "    cv2.imshow(\"Image Collection\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4dd6eb-1dcc-4b7d-ac44-3e815469d41a",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82dc597c-aabc-4efe-b5cf-088251a87c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESIZED_SHAPE = (105, 105)\n",
    "\n",
    "\n",
    "def preprocess(file_path):\n",
    "    byte_image = tf.io.read_file(file_path)\n",
    "    image = tf.io.decode_jpeg(byte_image)\n",
    "    image = tf.image.resize(image, RESIZED_SHAPE)\n",
    "    image = image / 255.0\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess_twin(input_image, validation_image, label):\n",
    "    return (preprocess(input_image), preprocess(validation_image), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f15b5cf-61cb-4f22-b7c8-0be716658bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Future exception was never retrieved\n",
      "future: <Future finished exception=BrokenPipeError(32, 'Broken pipe')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lohanyrvine/anaconda3/lib/python3.10/asyncio/unix_events.py\", line 676, in write\n",
      "    n = os.write(self._fileno, data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "anchor = tf.data.Dataset.list_files(os.path.join(ANC_PATH, \"*.jpg\")).take(400)\n",
    "positive = tf.data.Dataset.list_files(os.path.join(POS_PATH, \"*.jpg\")).take(400)\n",
    "negative = tf.data.Dataset.list_files(os.path.join(NEG_PATH, \"*.jpg\")).take(400)\n",
    "\n",
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7124382-e336-4a2e-bbde-872a53bbac54",
   "metadata": {},
   "source": [
    "### Build dataLoader pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4193fe30-42c3-453f-a936-41566377d9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90ef0a-0b29-4453-a463-6b1106cd2c75",
   "metadata": {},
   "source": [
    "### Training and testing partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e7c946-9bc6-4dd6-af57-75d64b875d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENTAGE_TAKEN = 0.7\n",
    "PERCENTAGE_NOT_TAKEN = 1.0 - PERCENTAGE_TAKEN\n",
    "\n",
    "train_data = data.take(round(len(data)*PERCENTAGE_TAKEN))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)\n",
    "\n",
    "test_data = data.skip(round(len(data)*PERCENTAGE_TAKEN))\n",
    "test_data = test_data.take(round(len(data)*PERCENTAGE_NOT_TAKEN))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de008587-6b10-4a14-aded-34261586023f",
   "metadata": {},
   "source": [
    "## Model Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd723ee-7bed-4d19-a614-08dbb1863d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "\n",
    "\n",
    "def make_embedding():\n",
    "    filters = 64\n",
    "    inp = Input(shape=(*RESIZED_SHAPE, 3), name=\"input_image\")\n",
    "    \n",
    "    convolution1 = Conv2D(filters, (10, 10), activation=\"relu\")(inp)\n",
    "    max_pooling1 = MaxPooling2D(filters, (2, 2), padding=\"same\")(convolution1)\n",
    "    \n",
    "    convolution2 = Conv2D(filters*2, (7, 7), activation=\"relu\")(max_pooling1)\n",
    "    max_pooling2 = MaxPooling2D(filters, (2, 2), padding=\"same\")(convolution2)\n",
    "    \n",
    "    convolution3 = Conv2D(filters*2, (4, 4), activation=\"relu\")(max_pooling2)\n",
    "    max_pooling3 = MaxPooling2D(filters, (2, 2), padding=\"same\")(convolution3)\n",
    "    \n",
    "    convolution4 = Conv2D(filters*4, (4, 4), activation=\"relu\")(max_pooling3)\n",
    "    flatten1 = Flatten()(convolution4)\n",
    "    dense1 = Dense(4096, activation=\"sigmoid\")(flatten1)\n",
    "    \n",
    "    return Model(inputs=[inp], outputs=[dense1], name=\"embedding\")\n",
    "\n",
    "\n",
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5de80b1-f4a1-4b4b-9339-64f4b8d27d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model(embedding):\n",
    "    input_image = Input(name=\"input_image\", shape=(*RESIZED_SHAPE, 3))\n",
    "    validation_image = Input(name=\"validation_image\", shape=(*RESIZED_SHAPE, 3))\n",
    "    \n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = \"distance\"\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "    \n",
    "    classifier = Dense(1, activation=\"sigmoid\")(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name=\"SiameseNetwork\")\n",
    "\n",
    "\n",
    "if not os.path.isfile(MODEL_PATH):\n",
    "    siamese_model = make_siamese_model(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff9acc9-971d-442b-bdf6-852546a48565",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fa3970f-b58b-45d7-84eb-5ac9537bc3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "opt = keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80ae3ca0-eee5-4405-9d28-3f8db2367227",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"training_checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PREFIX = os.path.join(CHECKPOINT_DIR, \"ckpt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b77b9e6a-ae2a-4a91-a0af-6e3202e599f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, batch):\n",
    "    binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        images = batch[:2]\n",
    "        label = batch[2]\n",
    "        \n",
    "        yhat = model(images, training=True)\n",
    "        loss = binary_cross_loss(label, yhat)\n",
    "    \n",
    "        # calculate gradients\n",
    "        grad = tape.gradient(loss, model.trainable_variables)\n",
    "        # calculate updated weights and apply to siamese model\n",
    "        opt.apply_gradients(zip(grad, model.trainable_variables))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c13fb9d8-5885-440e-998a-c40c68864249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        for idx, batch in enumerate(data):\n",
    "            train_step(siamese_model, batch)\n",
    "            progbar.update(idx+1)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint.save(file_prefix=CHECKPOINT_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "678b8741-ddd8-43cf-8757-10a9ecdbc1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fba304af-b45b-4d34-9d25-9b83993b69db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 20:36:43.666199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 23s 362ms/step\n",
      "\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 13s 375ms/step\n",
      "\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 13s 376ms/step\n",
      "\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 13s 379ms/step\n",
      "\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 13s 380ms/step\n",
      "\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 13s 380ms/step\n",
      "\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 13s 380ms/step\n",
      "\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 13s 380ms/step\n",
      "\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 13s 380ms/step\n",
      "\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 13s 380ms/step\n",
      "\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 13s 380ms/step\n",
      "\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 13s 384ms/step\n",
      "\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 13s 385ms/step\n",
      "\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 13s 385ms/step\n",
      "\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 14s 387ms/step\n",
      "\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 14s 386ms/step\n",
      "\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 14s 386ms/step\n",
      "\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 13s 385ms/step\n",
      "\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 13s 383ms/step\n",
      "\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 13s 383ms/step\n",
      "\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 13s 384ms/step\n",
      "\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 13s 382ms/step\n",
      "\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - 13s 380ms/step\n",
      "\n",
      "Epoch 34/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 35/50\n",
      "35/35 [==============================] - 13s 380ms/step\n",
      "\n",
      "Epoch 36/50\n",
      "35/35 [==============================] - 13s 380ms/step\n",
      "\n",
      "Epoch 37/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 38/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 39/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 40/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 41/50\n",
      "35/35 [==============================] - 13s 382ms/step\n",
      "\n",
      "Epoch 42/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 43/50\n",
      "35/35 [==============================] - 13s 380ms/step\n",
      "\n",
      "Epoch 44/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 45/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 46/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 47/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 48/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 49/50\n",
      "35/35 [==============================] - 13s 381ms/step\n",
      "\n",
      "Epoch 50/50\n",
      "35/35 [==============================] - 13s 381ms/step\n"
     ]
    }
   ],
   "source": [
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081b367-c317-4a0b-8192-db2f243e8c38",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f23ccb2-50d4-44ab-b4e0-5b9aa04a1877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 189ms/step\n",
      "Prediction: [0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Prediction: [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Prediction: [0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Prediction: [0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Prediction: [0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Prediction: [1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Prediction: [1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Prediction: [1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Prediction: [0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Prediction: [0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Prediction: [1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Prediction: [0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Prediction: [1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Prediction: [0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Prediction: [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_model(model, save_evaluation_imgs=False):\n",
    "    for i, (test_input, test_val, y_true) in enumerate(test_data.as_numpy_iterator()):\n",
    "        y_hat = model.predict([test_input, test_val])\n",
    "        filtered_y_hat = [1 if prediction > 0.5 else 0 for prediction in y_hat]\n",
    "        print(f\"Prediction: {filtered_y_hat}\")\n",
    "\n",
    "        mp = Precision()\n",
    "        mp.update_state(y_true, y_hat)\n",
    "        print(f\"Precision: {mp.result().numpy()}\")\n",
    "\n",
    "        mr = Recall()\n",
    "        mr.update_state(y_true, y_hat)\n",
    "        print(f\"Recall: {mr.result().numpy()}\\n\")\n",
    "\n",
    "        if save_evaluation_imgs:\n",
    "            batch_dir = os.path.join(\"evaluation_images\", f\"batch{i+1}\")\n",
    "            os.makedirs(batch_dir, exist_ok=True)\n",
    "            \n",
    "            for j in range(len(test_input)):\n",
    "                plt.figure(figsize=(16, 8))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(test_input[j])\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.imshow(test_val[j])\n",
    "                plt.savefig(os.path.join(batch_dir, f\"prediction{j+1}_result{filtered_y_hat[j]}.jpg\"))\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "evaluate_model(siamese_model, save_evaluation_imgs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f87e7-b99b-49b1-9105-072079c8d5ce",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb54d70e-03e2-433a-8f5e-02a963be8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = siamese_model.get_config()\n",
    "with keras.utils.custom_object_scope(custom_objects):\n",
    "    custom_model = keras.Model.from_config(config)\n",
    "\n",
    "    custom_model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "    custom_model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f401df2-7407-4f05-9d2c-723fc965498c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
